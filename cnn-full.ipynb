{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ee7a7-3c82-4127-94d4-92f3fc65b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7b8a8-9bf3-42bd-a9f3-d19e7072134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# implementation of full CNN forward and backward pass using numpy with class\n",
    "# -----------------------\n",
    "# Convolution Layer (forward and backward) \n",
    "# -----------------------\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        limit = 1 / np.sqrt(in_channels * kernel_size * kernel_size)\n",
    "        self.weights = np.random.uniform(-limit, limit,\n",
    "                                         (out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias = np.zeros((out_channels, 1))\n",
    "        self.dw = np.zeros_like(self.weights)\n",
    "        self.db = np.zeros_like(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        batch_size, C, H, W = x.shape\n",
    "        F, _, kH, kW = self.weights.shape\n",
    "\n",
    "        x_padded = np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding), (self.padding, self.padding)))\n",
    "        self.x_padded = x_padded\n",
    "\n",
    "        out_H = (H + 2 * self.padding - kH) // self.stride + 1\n",
    "        out_W = (W + 2 * self.padding - kW) // self.stride + 1\n",
    "\n",
    "        out = np.zeros((batch_size, F, out_H, out_W))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(F):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        region = x_padded[b, :, h_start:h_start + kH, w_start:w_start + kW]\n",
    "                        out[b, f, i, j] = np.sum(region * self.weights[f]) + self.bias[f]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        batch_size, C, H, W = self.x.shape\n",
    "        F, _, kH, kW = self.weights.shape\n",
    "        _, _, out_H, out_W = dout.shape\n",
    "\n",
    "        dx = np.zeros_like(self.x_padded)\n",
    "        self.dw.fill(0)\n",
    "        self.db.fill(0)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for f in range(F):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        region = self.x_padded[b, :, h_start:h_start + kH, w_start:w_start + kW]\n",
    "\n",
    "                        self.dw[f] += dout[b, f, i, j] * region\n",
    "                        self.db[f] += dout[b, f, i, j]\n",
    "                        dx[b, :, h_start:h_start + kH, w_start:w_start + kW] += dout[b, f, i, j] * self.weights[f]\n",
    "\n",
    "        if self.padding > 0:\n",
    "            dx = dx[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e666e6-3198-411e-bc92-1d431f5b6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# -----------------------\n",
    "# ReLU Activation\n",
    "# -----------------------\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "# -----------------------\n",
    "# Max Pooling Layer\n",
    "# -----------------------\n",
    "class MaxPool2D:\n",
    "    def __init__(self, size=2, stride=2):\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        batch_size, C, H, W = x.shape\n",
    "        out_H = (H - self.size) // self.stride + 1\n",
    "        out_W = (W - self.size) // self.stride + 1\n",
    "        out = np.zeros((batch_size, C, out_H, out_W))\n",
    "        self.max_indices = np.zeros_like(x)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(C):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        region = x[b, c, h_start:h_start + self.size, w_start:w_start + self.size]\n",
    "                        max_val = np.max(region)\n",
    "                        out[b, c, i, j] = max_val\n",
    "                        mask = (region == max_val)\n",
    "                        self.max_indices[b, c, h_start:h_start + self.size, w_start:w_start + self.size] += mask\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.zeros_like(self.x)\n",
    "        batch_size, C, out_H, out_W = dout.shape\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for c in range(C):\n",
    "                for i in range(out_H):\n",
    "                    for j in range(out_W):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        dx[b, c, h_start:h_start + self.size, w_start:w_start + self.size] += (\n",
    "                            self.max_indices[b, c, h_start:h_start + self.size, w_start:w_start + self.size] *\n",
    "                            dout[b, c, i, j]\n",
    "                        )\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825f87f-029d-4cc6-9a52-858e50a0e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(1, 1, 28, 28)  # One grayscale image\n",
    "y = np.array([3])  # assuming 3 classes\n",
    "\n",
    "# define the layers of the CNN\n",
    "conv = Conv2D(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "relu = ReLU()\n",
    "pool = MaxPool2D(size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdb1e3-4f96-40f2-8f1e-998ec2e983a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# -----------------------\n",
    "# Flatten Layer\n",
    "# -----------------------\n",
    "class Flatten:\n",
    "    def forward(self, x):\n",
    "        self.original_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.original_shape)\n",
    "\n",
    "# -----------------------\n",
    "# Dense Layer\n",
    "# -----------------------\n",
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.weights = np.random.randn(in_dim, out_dim) * 0.01\n",
    "        self.bias = np.zeros((1, out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, dout):\n",
    "        self.dw = self.x.T @ dout\n",
    "        self.db = np.sum(dout, axis=0, keepdims=True)\n",
    "        return dout @ self.weights.T\n",
    "    \n",
    "# -----------------------\n",
    "# Softmax + Cross-Entropy\n",
    "# -----------------------\n",
    "class SoftmaxCrossEntropy:\n",
    "    def forward(self, x, y_true):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        self.y_true = y_true\n",
    "        m = x.shape[0]\n",
    "        log_likelihood = -np.log(self.probs[range(m), y_true])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        m = self.y_true.shape[0]\n",
    "        dx = self.probs.copy()\n",
    "        dx[range(m), self.y_true] -= 1\n",
    "        dx /= m\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd4485-b0bd-4836-98fa-a74624178efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-92635c4e7518>:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  out[b, f, i, j] = np.sum(region * self.weights[f]) + self.bias[f]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1226\n",
      "Epoch 2, Loss: 0.5819\n",
      "Epoch 3, Loss: 0.2145\n",
      "Epoch 4, Loss: 0.1278\n",
      "Epoch 5, Loss: 0.0904\n",
      "Epoch 6, Loss: 0.0697\n",
      "Epoch 7, Loss: 0.0565\n",
      "Epoch 8, Loss: 0.0475\n",
      "Epoch 9, Loss: 0.0408\n",
      "Epoch 10, Loss: 0.0358\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(1, 1, 28, 28)  # One grayscale image\n",
    "y = np.array([3])  # assuming 3 classes\n",
    "\n",
    "# define the layers of the CNN\n",
    "conv = Conv2D(in_channels=1, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
    "relu = ReLU()\n",
    "pool = MaxPool2D(size=2, stride=2)\n",
    "flatten = Flatten()\n",
    "dense = Dense(in_dim=2 * 14 * 14, out_dim=10)\n",
    "loss_fn = SoftmaxCrossEntropy()\n",
    "\n",
    "lr = 0.01\n",
    "for epoch in range(10):\n",
    "    out = conv.forward(x)\n",
    "    out = relu.forward(out)\n",
    "    out = pool.forward(out)\n",
    "    out = flatten.forward(out)\n",
    "    out = dense.forward(out)\n",
    "    loss = loss_fn.forward(out, y)\n",
    "\n",
    "    dout = loss_fn.backward()\n",
    "    dout = dense.backward(dout)\n",
    "    dout = flatten.backward(dout)\n",
    "    dout = pool.backward(dout)\n",
    "    dout = relu.backward(dout)\n",
    "    dout = conv.backward(dout)\n",
    "\n",
    "    conv.weights -= lr * conv.dw\n",
    "    conv.bias -= lr * conv.db\n",
    "    dense.weights -= lr * dense.dw\n",
    "    dense.bias -= lr * dense.db\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
